---
title: "Data Science II Homework 4"
author: "Charlotte Abrams"
date: "4/21/2019"
output: github_document
---
___


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(lasso2)
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)
library(tree)
library(gbm)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(plotmo)
library(pdp)
library(lime)
```
## Question 1


### Part 1(a)
Fit a regression tree with lpsa as the response and the other variables as predictors.
Use cross-validation to determine the optimal tree size. Which tree size corresponds
to the lowest cross-validation error? Is this the same as the tree size obtained using
the 1 SE rule?

```{r}
data("Prostate")

#Summarize data
summary(Prostate)

set.seed(1)
ctrl <- trainControl(method = "cv")
tree.prostate <- rpart(formula = lpsa~., 
                       data = Prostate,
                       control = rpart.control(cp = 0.001))

rpart.plot(tree.prostate)

cpTable <- printcp(tree.prostate)

plotcp(tree.prostate)

#Prune the tree based on the cp table
minErr <- which.min(cpTable[,4])

#minimum cross-validation error
tree.prostate.prune <- prune(tree.prostate, cp = cpTable[minErr,1])

#1SE rule
tree.prostate.SE <- prune(tree.prostate, cp = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5],1][1])
```
- Cross-validation determined that 8 was the optimal tree size (this tree size corresponded to the lowest cross-validation error). This is different, however, than the tree size obtained using the 1SE rule, which chose 4 s the optimal tree size. 

### Part 1(b)
Create a plot of the final tree you choose. Pick one of the terminal nodes, and
interpret the information displayed.
```{r}
rpart.plot(tree.prostate.SE)
tree.prostate.SE
```
-  I decided to look at the terminal node labelled 10. We know this is a terminal node because of the *. The split criterion is lweight < 3.68886. We know there are 38 observations in this branch with a deviance of 21.507510. The mean lpsa for observations with lweight<3.68886 is 2.0330830.

### Part 1(c)
Perform bagging and report the variable importance.
```{r}
bagging.grid <- expand.grid(mtry = 8, 
                       splitrule = "variance",
                       min.node.size = 1:20) 

set.seed(1)

bagging <- train(lpsa~., Prostate, 
                method = "ranger",
                tuneGrid = bagging.grid,
                trControl = ctrl,
                importance = 'permutation')

ggplot(bagging, highlight = TRUE)

barplot(sort(ranger::importance(bagging$finalModel),
             decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

bagging$results[which.min(bagging$results[,5]),]
```
-  From the barplot, we can see that lcavol has the highest importance, followd by lweight, svi, pgg45, lbph, lcp, gleason, and finally age with the lowest. 


### Part 1(d)
Perform random forests and report the variable importance.
```{r}
rf.grid <- expand.grid(mtry = 1:7, 
                       splitrule = "variance",
                       min.node.size = 1:20) 

set.seed(1)

rf.fit <- train(lpsa~., Prostate,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl,
                importance = 'permutation')

ggplot(rf.fit, highlight = TRUE)

barplot(sort(ranger::importance(rf.fit$finalModel), 
             decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

rf.fit$results[which.min(rf.fit$results[,5]),]
```
- From the barplot, we can see that lcavol has the highest importance, followd by svi, lweight, lcp, pgg45, gleason, lbph, and finally age with the lowest. 

### Part 1(e)
Perform boosting and report the variable importance.
```{r}
gbm.grid <- expand.grid(n.trees = c(2000,3000,5000),
                        interaction.depth = 2:10, 
                        shrinkage = c(0.01,0.001,0.003,0.005),
                        n.minobsinnode = 1)

gbm.grid <- expand.grid(n.trees = c(2000,3000),
                        interaction.depth = 2:10, 
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)

set.seed(1)

gbm.fit <- train(lpsa ~., Prostate, 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 verbose = FALSE,
                 trControl = ctrl)

ggplot(gbm.fit, highlight = TRUE)

summary(gbm.fit$finalModel, 
        las = 2, 
        cBars = 19, 
        cex.names = 0.6)
```
- From the barplot, we can see that lcavol has the highest importance, followd by lweight, svi, pgg45, lcp, age, lbph, and finally gleason with the lowest. 

### Part 1(f)
Which of the above models will you select to predict PSA level? Explain.
```{r}
overall = resamples(list(rf = rf.fit, gbm = gbm.fit, bagging = bagging))
summary(overall)
```
-  From the summary, we can see that the bagging model has the lowest mean RMSE, therefore, I would select that model to predict PSA level.


## Question 2
```{r}
oj = OJ

set.seed(1)
rowTrain <- sample(1:nrow(oj), 800)
train <- oj[rowTrain,]
test <- oj[-rowTrain,]

ctrl <- trainControl(method = "repeatedcv")
```

### Part 2(a)
Fit a classification tree to the training set, with Purchase as the response and the
other variables as predictors. Use cross-validation to determine the tree size and
create a plot of the final tree. Predict the response on the test data. What is the test
classification error rate?
```{r}
set.seed(1)
rpart.class <- train(Purchase ~., OJ, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-7,-2, len = 50))),
                   trControl = ctrl,
                   metric = "Accuracy")
ggplot(rpart.class, highlight = T)
rpart.plot(rpart.class$finalModel)
```

### Part 2(b)
Perform random forests on the training set and report variable importance. What is the test error rate?
```{r}
rf.grid <- expand.grid(mtry = 1:10,
                       splitrule = "gini",
                       min.node.size = 1:6)
set.seed(1)

rf.class <- train(Purchase ~., OJ, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "Accuracy",
                trControl = ctrl,
                importance = 'permutation')
ggplot(rf.class, highlight = TRUE)

rf.pred = predict(rf.class, newdata = OJ[-rowTrain,])

error_rate_rf <- mean(rf.pred != OJ$Purchase[-rowTrain])
```
-  The test error rate is 0.2

### Part 2(c)
Perform boosting on the training set and report variable importance. What is the test error rate?
```{r}
boosting.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:6,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)

set.seed(1)

boosting.fit <- train(Purchase ~., oj, 
                 subset = rowTrain, 
                 tuneGrid = boosting.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "Accuracy",
                 verbose = FALSE)

ggplot(boosting.fit, highlight = TRUE)

gbm.pred = predict(boosting.fit, newdata = OJ[-rowTrain,])

error_rate_boost <- mean(gbm.pred != OJ$Purchase[-rowTrain])
```
-  The test error rate is 0.1962